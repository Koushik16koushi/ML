Experiment 5
Develop a program to implement k-Nearest Neighbour
algorithm to classify the randomly generated 100 values of
x in the range of [0,1]. Perform the following based on
dataset generated.
1. Label the first 50 points {x1,‚Ä¶‚Ä¶,x50} as follows: if (xi ‚â§
0.5), then xi Œµ Class1, else xi Œµ Class2
2. Classify the remaining points, x51,‚Ä¶‚Ä¶,x100 using KNN.
Perform this for k=1,2,3,4,5,20,30

Introduction to k-Nearest Neighbors (k-NN)
What is k-NN?
The k-Nearest Neighbors (k-NN) algorithm is a supervised learning algorithm used
for both classification and regression. It classifies a data point based on the majority
class among its nearest neighbors.
It is also called a lazy learner algorithm because it does not learn from the training set
immediately instead it stores the dataset and at the time of classification, it performs an
action on the dataset

Importance of k-NN
Simple and effective for classification tasks.
Non-parametric (makes no assumptions about the data distribution).
Handles multi-class classification with ease.

How k-NN Works?
The k-NN algorithm follows these steps:
1. Choose the value of k (number of nearest neighbors).
2. Compute the distance between the test sample and all training samples using a
distance metric (e.g., Euclidean distance).
3. Select the k nearest neighbors (data points with the smallest distance to the test
sample).
4. Assign the majority class among the k neighbors to the test sample.

36

Working of the k-NN Algorithm
1. Choose a Value for k:
A small k (e.g., k=1) makes the model sensitive to noise and results in high variance.
A large k (e.g., k=30) smooths the decision boundary but may lead to high bias.
The optimal k is usually found by cross-validation.
2. Compute Distance Between Data Points: The algorithm relies on a distance metric to
determine similarity between data points. Common distance measures include:
`- Euclidean Distance (Most commonly used)
Manhattan Distance
Minkowski Distance
`Cosine Similarity** (Used in text-based applications)
The most common method is Euclidean Distance:
d =‚àö [(x2 ‚Äì x1)2 + (y2 ‚Äì y1)2]
3. 3. Decision Rule for Classification
Majority Voting: The most common class among the k neighbors determines the
predicted class.
Weighted Voting: Closer neighbors have higher influence on the prediction than
farther neighbors.

Dataset Generation and Classification Task
Step 1: Generate 100 Random Points in the Range [0,1]
The dataset consists of 100 random values of x uniformly distributed between 0 and
1.

Step 2: Assign Labels to the First 50 Points
The first 50 points (x‚ÇÅ, x‚ÇÇ, ..., x‚ÇÖ‚ÇÄ) are labeled as:
Class 1 if x_i <= 0.5
Class 2 if x_i > 0.5

Step 3: Classify Remaining Points (x‚ÇÖ‚ÇÅ, ..., x‚ÇÅ‚ÇÄ‚ÇÄ) using k-NN
The k-NN algorithm is used to classify the next 50 points based on the first 50 labeled
points.

Step 4: Experiment with Different k Values

37

Classification is performed for multiple values of k :
k = 1, 2, 3, 4, 5, 20, 30
Observing how different values of k affect classification accuracy and decision
boundaries.

Effect of Different k Values
k Value

Effect on Classification

k=1

Highly sensitive to noise; can result in overfitting.

k = 2, 3, 4, 5

Balanced classification with some noise handling.

k = 20, 30

Smoother decision boundary but may lead to underfitting.

Bias-Variance Tradeoff in k-NN
Smaller k values (e.g., k=1) ‚Üí Low bias, high variance (more flexible but prone to
noise).
Larger k values (e.g., k=20, 30) ‚Üí High bias, low variance (less flexible but smoother
decision boundary).

Advantages of k-NN
‚úî Simple and easy to implement.
‚úî No training phase‚Äîall computation happens during prediction.
‚úî Works well for multi-class classification problems.
‚úî Can model complex decision boundaries when k is appropriately chosen.

Limitations of k-NN
‚ùå Computationally expensive for large datasets.
‚ùå Performance depends on the choice of k.
‚ùå Sensitive to irrelevant or redundant features.
‚ùå Memory-intensive since all training data needs to be stored.
Problem Explanation
The goal is to implement the k-Nearest Neighbors (KNN) algorithm to classify 100 randomly
generated values in the range [0,1]. The classification process involves the following steps:
Generating the Dataset:
Create 100 random values in the range [0,1] The first 50 values are manually labeled based
on a given rule: If ùë•ùëñ‚â§0.5 assign it to Class1.Otherwise, assign it to Class2. Classifying the

38

Remaining 50 Values Using KNN:
The next 50 values (ùë•51‚Äãto ùë•100) are unlabeled. We use the KNN algorithm to classify these
values based on their nearest neighbors among the first 50 labeled points.
Perform classification for k = 1, 2, 3, 4, 5, 20, 30.
In [1]: import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings('ignore')
In [2]: # Step 1: Generate dataset
np.random.seed(42)
values = np.random.rand(100)
In [3]: labels = []
for i in values[:50]:
if i <=0.5:
labels.append('Class1')
else:
labels.append('Class2')

In [4]: labels += [None] * 50
In [5]: print(labels)
['Class1', 'Class2', 'Class2', 'Class2', 'Class1', 'Class1', 'Class1', 'Class2', 'Cl
ass2', 'Class2', 'Class1', 'Class2', 'Class2', 'Class1', 'Class1', 'Class1', 'Class
1', 'Class2', 'Class1', 'Class1', 'Class2', 'Class1', 'Class1', 'Class1', 'Class1',
'Class2', 'Class1', 'Class2', 'Class2', 'Class1', 'Class2', 'Class1', 'Class1', 'Cla
ss2', 'Class2', 'Class2', 'Class1', 'Class1', 'Class2', 'Class1', 'Class1', 'Class
1', 'Class1', 'Class2', 'Class1', 'Class2', 'Class1', 'Class2', 'Class2', 'Class1',
None, None, None, None, None, None, None, None, None, None, None, None, None, None,
None, None, None, None, None, None, None, None, None, None, None, None, None, None,
None, None, None, None, None, None, None, None, None, None, None, None, None, None,
None, None, None, None, None, None, None, None]
In [6]: data = {
"Point": [f"x{i+1}" for i in range(100)],
"Value": values,
"Label": labels
}
In [11]: df = pd.DataFrame(data)

39

df.head()
Point

Value

Label

0

x1

0.374540

Class1

1

x2

0.950714

Class2

2

x3

0.731994

Class2

3

x4

0.598658

Class2

4

x5

0.156019

Class1

Out[11]:

In [8]: # Table of Meaning of Each Variable
variable_meaning = {
"Point": "The point number",
"Value": "The value of the point",
"Label": "The class of the point"
}
variable_df = pd.DataFrame(list(variable_meaning.items()), columns=["Feature", "Des
print("\nVariable Meaning Table:")
print(variable_df)
Variable Meaning Table:
Feature
Description
0
Point
The point number
1
Value The value of the point
2
Label The class of the point
In [12]: df.nunique()
Out[12]:

Point
100
Value
100
Label
2
dtype: int64

In [13]: df.shape
Out[13]:

(100, 3)

In [16]: # Basic Data Exploration
print("\nBasic Information about Dataset:")
df.info()
Basic Information about Dataset:
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 100 entries, 0 to 99
Data columns (total 3 columns):
#
Column Non-Null Count Dtype
--- ------ -------------- ----0
Point
100 non-null
object
1
Value
100 non-null
float64
2
Label
50 non-null
object
dtypes: float64(1), object(2)
memory usage: 2.5+ KB

40

In [15]: print("\nSummary Statistics:")
df.describe().T
Summary Statistics:
Out[15]:
Value

count

mean

std

min

25%

50%

75%

max

100.0

0.470181

0.297489

0.005522

0.193201

0.464142

0.730203

0.986887

In [25]: Summary_Statistics="""
- The 'Value' column has a mean of approximately 0.47, indicating that the values a
- The standard deviation of the 'Value' column is approximately 0.29, showing a mod
- The minimum value in the 'Value' column is approximately 0.0055, and the maximum
- The first quartile (25th percentile) is approximately 0.19, the median (50th perc
print(Summary_Statistics)
- The 'Value' column has a mean of approximately 0.47, indicating that the values ar
e uniformly distributed.
- The standard deviation of the 'Value' column is approximately 0.29, showing a mode
rate spread around the mean.
- The minimum value in the 'Value' column is approximately 0.0055, and the maximum v
alue is approximately 0.9869.
- The first quartile (25th percentile) is approximately 0.19, the median (50th perce
ntile) is approximately 0.47, and the third quartile (75th percentile) is approximat
ely 0.73.
In [17]: # Check for missing values
print("\nMissing Values in Each Column:")
df.isnull().sum()
Missing Values in Each Column:
Out[17]:

Point
0
Value
0
Label
50
dtype: int64

In [16]: # Get numeric columns
num_col = df.select_dtypes(include=['int', 'float']).columns
# Histograms for distribution of features
df[num_col].hist(figsize=(12, 8), bins=30, edgecolor='black')
# Title and labels
plt.suptitle("Feature Distributions", fontsize=16)
plt.show()

41

In [19]: # Inference for the above graph
inference = """
- The histograms for the distribution of features show that the values are uniforml
- This is expected as the values were generated using a uniform random distribution
- There are no significant outliers or skewness in the data, indicating that the da
"""
print(inference)
- The histograms for the distribution of features show that the values are uniformly
distributed across the range [0, 1].
- This is expected as the values were generated using a uniform random distribution.
- There are no significant outliers or skewness in the data, indicating that the dat
aset is well-balanced.

In [17]: # Split data into labeled and unlabeled
labeled_df = df[df["Label"].notna()]
X_train = labeled_df[["Value"]]
y_train = labeled_df["Label"]
In [92]: unlabeled_df = df[df["Label"].isna()]
X_test = unlabeled_df[["Value"]]
In [93]: # Generate true labels for testing (for accuracy calculation)
true_labels = ["Class1" if x <= 0.5 else "Class2" for x in values[50:]]

42

In [94]: # Step 2: Perform KNN classification for different values of k
k_values = [1, 2, 3, 4, 5, 20, 30]
results = {}
accuracies = {}
In [96]: for k in k_values:
knn = KNeighborsClassifier(n_neighbors=k)
knn.fit(X_train, y_train)
predictions = knn.predict(X_test)
results[k] = predictions
# Calculate accuracy
accuracy = accuracy_score(true_labels, predictions) * 100
accuracies[k] = accuracy
print(f"Accuracy for k={k}: {accuracy:.2f}%")
# Assign predictions back to the DataFrame for this k
unlabeled_df[f"Label_k{k}"] = predictions
Accuracy for k=1: 100.00%
Accuracy for k=2: 100.00%
Accuracy for k=3: 98.00%
Accuracy for k=4: 98.00%
Accuracy for k=5: 98.00%
Accuracy for k=20: 98.00%
Accuracy for k=30: 100.00%
In [26]: # Inference for the KNN classification results
knn_inference = """
- The KNN classification was performed for different values of k: 1, 2, 3, 4, 5, 20
- The accuracy of the classification varied with the value of k.
- For smaller values of k (1, 2, 3, 4, 5), the accuracy was relatively high, indica
- As the value of k increased to 20 and 30, the accuracy decreased, suggesting that
- This is expected as higher values of k can lead to over-smoothing, where the mode
- Overall, the KNN classifier performed well for smaller values of k, with the high
"""
print(knn_inference)
- The KNN classification was performed for different values of k: 1, 2, 3, 4, 5, 20,
and 30.
- The accuracy of the classification varied with the value of k.
- For smaller values of k (1, 2, 3, 4, 5), the accuracy was relatively high, indicat
ing that the model was able to classify the points correctly.
- As the value of k increased to 20 and 30, the accuracy decreased, suggesting that
the model's performance deteriorated with higher values of k.
- This is expected as higher values of k can lead to over-smoothing, where the model
becomes less sensitive to the local structure of the data.
- Overall, the KNN classifier performed well for smaller values of k, with the highe
st accuracy observed for k=1.

In [97]: print(predictions)

43

['Class2' 'Class2' 'Class2' 'Class2' 'Class2' 'Class2' 'Class1' 'Class1'
'Class1' 'Class1' 'Class1' 'Class1' 'Class2' 'Class1' 'Class1' 'Class2'
'Class1' 'Class2' 'Class1' 'Class2' 'Class2' 'Class1' 'Class1' 'Class2'
'Class2' 'Class2' 'Class2' 'Class1' 'Class1' 'Class1' 'Class2' 'Class2'
'Class1' 'Class1' 'Class1' 'Class1' 'Class2' 'Class2' 'Class2' 'Class1'
'Class1' 'Class2' 'Class2' 'Class2' 'Class2' 'Class1' 'Class2' 'Class1'
'Class1' 'Class1']
In [98]: df1 = unlabeled_df.drop(columns=['Label'], axis=1)
df1

44

Point

Value

Label_k1

Label_k2

Label_k3

Label_k4

Label_k5

Label_k20

Label_k3

50

x51

0.969585

Class2

Class2

Class2

Class2

Class2

Class2

Class

51

x52

0.775133

Class2

Class2

Class2

Class2

Class2

Class2

Class

52

x53

0.939499

Class2

Class2

Class2

Class2

Class2

Class2

Class

53

x54

0.894827

Class2

Class2

Class2

Class2

Class2

Class2

Class

54

x55

0.597900

Class2

Class2

Class2

Class2

Class2

Class2

Class

55

x56

0.921874

Class2

Class2

Class2

Class2

Class2

Class2

Class

56

x57

0.088493

Class1

Class1

Class1

Class1

Class1

Class1

Class

57

x58

0.195983

Class1

Class1

Class1

Class1

Class1

Class1

Class

58

x59

0.045227

Class1

Class1

Class1

Class1

Class1

Class1

Class

59

x60

0.325330

Class1

Class1

Class1

Class1

Class1

Class1

Class

60

x61

0.388677

Class1

Class1

Class1

Class1

Class1

Class1

Class

61

x62

0.271349

Class1

Class1

Class1

Class1

Class1

Class1

Class

62

x63

0.828738

Class2

Class2

Class2

Class2

Class2

Class2

Class

63

x64

0.356753

Class1

Class1

Class1

Class1

Class1

Class1

Class

64

x65

0.280935

Class1

Class1

Class1

Class1

Class1

Class1

Class

65

x66

0.542696

Class2

Class2

Class2

Class2

Class2

Class2

Class

66

x67

0.140924

Class1

Class1

Class1

Class1

Class1

Class1

Class

67

x68

0.802197

Class2

Class2

Class2

Class2

Class2

Class2

Class

68

x69

0.074551

Class1

Class1

Class1

Class1

Class1

Class1

Class

69

x70

0.986887

Class2

Class2

Class2

Class2

Class2

Class2

Class

70

x71

0.772245

Class2

Class2

Class2

Class2

Class2

Class2

Class

71

x72

0.198716

Class1

Class1

Class1

Class1

Class1

Class1

Class

72

x73

0.005522

Class1

Class1

Class1

Class1

Class1

Class1

Class

73

x74

0.815461

Class2

Class2

Class2

Class2

Class2

Class2

Class

74

x75

0.706857

Class2

Class2

Class2

Class2

Class2

Class2

Class

75

x76

0.729007

Class2

Class2

Class2

Class2

Class2

Class2

Class

76

x77

0.771270

Class2

Class2

Class2

Class2

Class2

Class2

Class

77

x78

0.074045

Class1

Class1

Class1

Class1

Class1

Class1

Class

78

x79

0.358466

Class1

Class1

Class1

Class1

Class1

Class1

Class

79

x80

0.115869

Class1

Class1

Class1

Class1

Class1

Class1

Class

Out[98]:

45

Point

Value

Label_k1

Label_k2

Label_k3

Label_k4

Label_k5

Label_k20

Label_k3

80

x81

0.863103

Class2

Class2

Class2

Class2

Class2

Class2

Class

81

x82

0.623298

Class2

Class2

Class2

Class2

Class2

Class2

Class

82

x83

0.330898

Class1

Class1

Class1

Class1

Class1

Class1

Class

83

x84

0.063558

Class1

Class1

Class1

Class1

Class1

Class1

Class

84

x85

0.310982

Class1

Class1

Class1

Class1

Class1

Class1

Class

85

x86

0.325183

Class1

Class1

Class1

Class1

Class1

Class1

Class

86

x87

0.729606

Class2

Class2

Class2

Class2

Class2

Class2

Class

87

x88

0.637557

Class2

Class2

Class2

Class2

Class2

Class2

Class

88

x89

0.887213

Class2

Class2

Class2

Class2

Class2

Class2

Class

89

x90

0.472215

Class1

Class1

Class1

Class1

Class1

Class1

Class

90

x91

0.119594

Class1

Class1

Class1

Class1

Class1

Class1

Class

91

x92

0.713245

Class2

Class2

Class2

Class2

Class2

Class2

Class

92

x93

0.760785

Class2

Class2

Class2

Class2

Class2

Class2

Class

93

x94

0.561277

Class2

Class2

Class2

Class2

Class2

Class2

Class

94

x95

0.770967

Class2

Class2

Class2

Class2

Class2

Class2

Class

95

x96

0.493796

Class1

Class1

Class2

Class2

Class2

Class2

Class

96

x97

0.522733

Class2

Class2

Class2

Class2

Class2

Class2

Class

97

x98

0.427541

Class1

Class1

Class1

Class1

Class1

Class1

Class

98

x99

0.025419

Class1

Class1

Class1

Class1

Class1

Class1

Class

99

x100

0.107891

Class1

Class1

Class1

Class1

Class1

Class1

Class

In [99]: # Display accuracies
print("\nAccuracies for different k values:")
for k, acc in accuracies.items():
print(f"k={k}: {acc:.2f}%")
Accuracies for different k values:
k=1: 100.00%
k=2: 100.00%
k=3: 98.00%
k=4: 98.00%
k=5: 98.00%
k=20: 98.00%
k=30: 100.00%

Key Insights:
The KNN classification was performed for different values of k: 1, 2, 3, 4, 5, 20, and 30.

46

The accuracy of the classification varied with the value of k.
For smaller values of k (1, 2, 3, 4, 5), the accuracy was relatively high, indicating that the
model was able to classify the points correctly.
As the value of k increased to 20 and 30, the accuracy decreased, suggesting that the
model's performance deteriorated with higher values of k.
This is expected as higher values of k can lead to over-smoothing, where the model
becomes less sensitive to the local structure of the data.
Overall, the KNN classifier performed well for smaller values of k, with the highest
accuracy observed for k=1.

47

